[
  {
    "objectID": "projects/phd-thesis.html",
    "href": "projects/phd-thesis.html",
    "title": "Memory and sampling in contextual judgment",
    "section": "",
    "text": "Abstract\nThis thesis investigates the interaction of memory and decision making in relative and retrospective judgment. Theories of memory and decision making are often rigorously tested using a variety of data sets, and the resulting theories can be applied to a large selection of psychological phenomena. In Chapter 1 I argue that theoretical development in relative and retrospective judgment is in contrast often very specialized. Theories of relative and retrospective judgment cannot easily be applied to other memory and decision making phenomena. Another approach is to take broad models or principles from the wider literature and apply them to relative and retrospective judgment. I suggest that the SIMPLE model of memory (Brown, Neath, & Chater, 2007) and the decision by sampling model (DbS; Stewart, Chater, & Brown, 2006) can in combination offer a comprehensive and unifying account of relative judgment. In Chapter 2 I find that both relative and retrospective judgments are consistent with range-frequency theory. I also find evidence for range effects which are inconsistent with decision by sampling. Chapter 3 investigates the role of similarity in these relative judgments using the distance based sampling model (Qian & Brown, 2005). The results show no evidence for distance based sampling. A combined SIMPLE and DbS model (SDbS) is applied to data from previous studies in Chapter 4. SDbS and range-frequency theory can account for the data – including range effects - equally well. In Chapter 5 I use an incentivized free recall task to elicit atypical serial position curves in three experiments. SIMPLE is shown to be able to fit the effect of output position which appears important in decision making behavior. Overall, this thesis suggests that SDbS is a candidate model for unifying retrospective and relative judgment with the wider memory and decision making literature."
  },
  {
    "objectID": "projects/qstep-sql.html",
    "href": "projects/qstep-sql.html",
    "title": "SQL Masterclass",
    "section": "",
    "text": "Designing and delivering this workshop was a lot of fun. The goal was to introduce Structured Query Language. Students were provided with a containerised Linux machine so they could work with the SQL in a live environment (the Ansible playbook used can be seen here).\nTwo datasets were imported in the database. The first was a rather nice geospatial dataset containing the world borders. The second were world development indicators available via the world bank - superb datasource! Full details are here.\nStudents then (a) carried out basic SQL queries, (b) aggregated and joined data, (c) imported the data from the database into R via an SSH tunnel and RPostgreSQL to create choropleths, and (d) explored additional datasets.\nFeedback from students was very good with this one. I may even have encouraged some aspiring data scientists :)."
  },
  {
    "objectID": "projects/negativelyskewed-paper.html",
    "href": "projects/negativelyskewed-paper.html",
    "title": "Being paid relatively well most of the time",
    "section": "",
    "text": "A paper from my PhD thesis.\n\nAbstract\nHow does the structure of a series of payments influence its recipient’s satisfaction? A common hypothesis is that each payment will be compared with a single “standard” or “reference” payment (e.g., the average payment). Cognitive models of judgment such as range frequency theory predict in contrast that the entire payment distribution will influence evaluation of each individual payment. Two experiments examined satisfaction with a series of payments. In both experiments, most payments were either relatively high in the experienced distribution (the distribution was negatively skewed) or relatively low (positively skewed). The total and average payment was held constant. Experiment 1 found that average satisfaction with individual payments was higher when the payments were negatively skewed, consistent with range frequency theory, and earlier findings were extended by comparing range frequency theory with a range-based model, a rank-based model, and a reference point model at the individual level. Experiment 2 examined satisfaction with whole sequences of payments and found that receiving a negatively skewed sequence was more satisfying overall than receiving a positively skewed sequence. It is concluded that negatively skewed payment distributions are more satisfying, as predicted by cognitive models of judgment."
  },
  {
    "objectID": "projects/networking.html",
    "href": "projects/networking.html",
    "title": "Networking Workshop",
    "section": "",
    "text": "I worked with Godwin to develop an introductory network analysis skills workshop. It was a great chance to work with rmarkdown’s render to github markdown option along with the rather superb references support via Zotero.\nThe workshop was delivered to faculty members from the History department at the University of Warwick. We used an existing dataset of quakers and carried out our analysis using Grapho - a tool I often used during my time at the Centre for Interdisciplinary Methodologies.\nI spent a good period of time diving into the relevent literature and the papers which made the cut (rather good papers) are linked to below as well.\n\nOutline\nDo you want to learn a new skill as a historian? The Information and Digital Group Technology for Research (IDGT4R) team has been asked to lead an introductory practitioners’ approach on the applications of social networks as a complimentary 90-minute component to a composite session (Networks: A Skills Workshop). This practical part to be preceded by a talk from Kate Davidson (Sheffield) on Social Network Analysis from her article on Early Modern Social Networks: Antecedents, Opportunities, and Challenges in American Historical Review.\n\n\nLiterature\nAhnert, R., Ahnert, S. E., Coleman, C. N., & Weingart, S. B. (2020). The Network Turn: Changing Perspectives in the Humanities. Elements in Publishing and Book Culture. https://doi.org/10.1017/9781108866804\nConroy, M. (2021). Networks, Maps, and Time: Visualizing Historical Networks Using Palladio. Digital Humanities Quarterly, 015(1).\nDonnellan, L. (2019). Modeling the Rise of the City: Early Urban Networks in Southern Italy. Frontiers in Digital Humanities, 6. https://www.frontiersin.org/article/10.3389/fdigh.2019.00015\nFinegold, M., Otis, J., Shalizi, C., Shore, D., Wang, L., & Warren, C. (2016). Six Degrees of Francis Bacon: A Statistical Method for Reconstructing Large Historical Social Networks. Digital Humanities Quarterly, 10(3). https://hcommons.org/deposits/item/mla:989/\nGrandjean, M. (2016). A social network analysis of Twitter: Mapping the digital humanities community. Cogent Arts & Humanities, 3(1), 1171458. https://doi.org/10.1080/23311983.2016.1171458\nJacomy, M., Venturini, T., Heymann, S., & Bastian, M. (2014). ForceAtlas2, a Continuous Graph Layout Algorithm for Handy Network Visualization Designed for the Gephi Software. PLOS ONE, 9(6), e98679. https://doi.org/10.1371/journal.pone.0098679\nMarres, N. (2015). Why Map Issues? On Controversy Analysis as a Digital Method. Science, Technology, & Human Values, 40(5), 655–686. https://doi.org/10.1177/0162243915574602\nRyan, Y. C., & Ahnert, S. E. (2021). The measure of the archive: The robustness of network analysis in early modern correspondence. Journal of Cultural Analytics, 6(3), 25943.\nTommaso, V., Jacomy, M., & Jensen, P. (2021). What do we see when we look at networks: Visual network analysis, relational ambiguity, and force-directed layouts. Big Data & Society, 8(1). https://doi.org/10.1177/20539517211018488\nVenturini, T., Bounegru, L., Jacomy, M., & Gray, J. (2017). How to tell stories with networks: Exploring the narrative affordances of graphs with the Iliad. In The Datafied Society: Studying Culture through Data (pp. 155–170). Amsterdam University Press."
  },
  {
    "objectID": "projects/qstep-social-media.html",
    "href": "projects/qstep-social-media.html",
    "title": "Masterclass: Social Media",
    "section": "",
    "text": "Code on GitHub\nRStudio:\nI was asked to deliver a Social Media Data Analysis workshop in R by the QSTEP centre at the University of Warwick. I decided to write the Masterclass entirely, where possible, using the tidyverse packages. It was a delightful learning experience and I would suggest anyone face the challenge of putting together such a workshop.\nThe workshop covered methods for downloading, analysing and visualising social media data using the R programming language. We use the ‘tidyverse’ in R and (optionally) the spacy python module for natural language processing."
  },
  {
    "objectID": "projects/qstep-social-media.html#outline",
    "href": "projects/qstep-social-media.html#outline",
    "title": "Masterclass: Social Media",
    "section": "Outline",
    "text": "Outline\nThe structure of the workshop is as follows\n\n\n\nStage\nTitle\nDetail\nR package(s)\n\n\n\n\n\nIntroduction\nOverview of the day\n\n\n\n\nR intro\nAn introduction to R\nggplot2, tidyverse\n\n\nCollection\nScraping\nDownloading and filtering html pages\nrvest, tidyverse, magittr, ggplot2, tibble\n\n\n\nAPI and data dumps\nAccessing data directly using APIs\nhttr, jsonlite, dplyr, textclean, stringr, ggplot2, tidyverse, magittr, tibble, twitteR, RedditExtractoR\n\n\nAnalysis\nSummarising\nTidyverse enabled summaries of our collected data\ntidyverse, tidytext, dplyr, tidyr\n\n\n\nText analysis\nApplying numerical analysis to our text\ntidytext, tidyverse, dplyr, stringr, RedditExtractoR, tidyr, igraph, ggraph, wordcloud, reshape2, tm, topicmodels\n\n\n\nNatural Language\nOptional section using the cleanNLP package\ncleanNLP, tibble, tidyverse, RedditExtractoR, reticulate"
  },
  {
    "objectID": "projects/grapho.html",
    "href": "projects/grapho.html",
    "title": "Grapho",
    "section": "",
    "text": "I wrote this software alongside Greg, a fantastic visualisation academic, and it remains unfinished. The software uses a callback function to record commands passed to the R interpreter into a text file and records the plots created. All of these are then saved with date annotation.\nI moved to CIM part way through a rewrite of the package loading process. If anyone would like to fork or futher develop the software then please do get in touch.\n\nProject Description\nGrapho is an R package for recording the commands and visualisations created in an R session. Functions are provided for parsing and visualising the user workflow.\nGrapho is part of WAYS: What aren’t you seeing. The goal of this Turing funded research project is to ‘develop tools which enhance people’s capacity to visualise data, by letting them see what can and can’t be seen in the visualisation.’.\nThe version in this repository is currently under active development. A full release is forthcoming. Please see the roadmap for more details.\nThe package is written by Dr James Tripp and Dr Greg McInerny. Please contact James with any technical issues."
  },
  {
    "objectID": "projects/backfillz.html",
    "href": "projects/backfillz.html",
    "title": "Backfillz",
    "section": "",
    "text": "This software provides several novel visualisations for exploring MCMC chains. I worked with MCMC chains during the my post-doc in Psychology when estimating the posterior distributions of cognitive models given the data and found this project very interesting. The software has been released and is considered complete. Fun stuff.\n\nProject Description\nBackfillz.R provides new visual diagnostics for understanding MCMC (Markov Chain Monte Carlo) analyses and outputs. MCMC chains can defy a simple line graph. Unless the chain is very short (which isn’t often the case), plotting tens or hundreds of thousands of data points reveals very little other than a ‘trace plot’ where we only see the outermost points. Common plotting methods may only reveal when an MCMC really hasn’t worked, but not when it has. BackFillz.R slices and dices MCMC chains so increasingly parameter rich, complex analyses can be visualised meaningfully. What does ‘good mixing’ look like? Is a ‘hair caterpillar’ test verifiable? What does a density plot show and what does it hide?"
  },
  {
    "objectID": "projects/quarto-rseconf.html",
    "href": "projects/quarto-rseconf.html",
    "title": "Quarto: a library to run them all?",
    "section": "",
    "text": "Slides\n\n\n\n\nAbstract\nUsing literate programming is a widespread practice amongst data scientists. This practice not only encourages data scientists to produce transparent, rich and reflective accounts of their analysis without the extra overhead of switching between tools, but also leads to artefacts (i.e., notebooks) that are increasingly becoming a medium for dissemination, reproducibility and education. Rmarkdown or Jupyter notebooks, are two of the most well known and used options. While both solutions can be used with multiple programming languages, the decision of whether to use one or the other is almost certain to be exclusively based on that. At least until now, with Quarto being mature enough to become a game-changer.\nQuarto is a language-agnostic software based on Pandoc to render files combining markdown and code into multiple ranges of formats and outputs. As a result, it can be used with either R, Python or Julia without any other dependencies.\nThis collaborative workshop will be structured as follows:\n\na brief theoretical introduction and instructions;\na task that participants may choose from the different use cases provided (i.e. generating a single document, migrating from Rmarkdown or Jupyter, creating a book or generating interactive content); and\na group discussion and conclusions.\n\nParticipants in this workshop have fun while gaining enough depth of breath and practice to evaluate how feasible it is to use Quarto in different scenarios and, ultimately, if it can become the one tool for authoring reproducible scientific or technical documents, regardless of your language of choice."
  },
  {
    "objectID": "projects/dilutioneffect-paper.html",
    "href": "projects/dilutioneffect-paper.html",
    "title": "A dilution effect without dilution",
    "section": "",
    "text": "A paper from my post-doc.\n\nAbstract\nWhen asked to combine two pieces of evidence, one diagnostic and one non-diagnostic, people show a dilution effect: the addition of non-diagnostic evidence dilutes the overall strength of the evidence. This non-normative effect has been found in a variety of tasks and has been taken as evidence that people inappropriately combine information. In a series of five experiments, we found the dilution effect, but surprisingly it was not due to the inaccurate combination of diagnostic and non-diagnostic information. Because we have objectively correct answers for our task, we could see that participants were relatively accurate in judging diagnostic evidence combined with non-diagnostic evidence, but overestimated the strength of diagnostic evidence alone. This meant that the dilution effect – the gap between diagnostic evidence alone and diagnostic evidence combined with non-diagnostic evidence – was not caused by dilution. We hypothesized that participants were filling in “missing” evidence in a biased fashion when presented with diagnostic evidence alone. This hypothesis best explained the experimental results."
  },
  {
    "objectID": "projects/lecat.html",
    "href": "projects/lecat.html",
    "title": "LE-CAT: Lexicon-based Categorization and Analysis Tool",
    "section": "",
    "text": "I developed LE-CAT for Professor Noortje Marres. The software is written in R and searches for words in a text. Each word is associated with a category. The presence of the word (e.g., Tony Blair) is assumed to indicate the presence of the category in the text (e.g., politics). Summary information about category co-occurence is displayed displayed to the user as is a network of category and word co-occurence.\nAn interesting feature of the software is that it is an R package containing a shiny app. I could teach with the tool to users who (a) were comfortable with R, (b) preferred a GUI interface running within R (the shiny app), and (c) much preferred only a web browser interface. At CIM I ran LE-CAT within docker containers via the shinyproxy application so that each user could log into one of our CIM servers and use LE-CAT without R installed.\nThe software was used for both research and in teaching environments.\n\nProject Description\nLE-CAT is a Lexicon-based Categorization and Analysis Tool developed by the Centre for Interdisciplinary Methodologies in collaboration with the Media of Cooperation Group at the University of Siegen.\nThe tool allows you to apply a set of word queries associated with a category (a lexicon) to a data set of textual sources (the corpus). LE-CAT determines the frequency of occurrence for each query and category in the corpus, as well as the relations between categories (co-occurrence) by source.\nThe purpose of this technique is to automate and scale up user-led data analysis as it allows the application of a custom-built Lexicon to large data sets. The quick iteration of analysis allows the user to refine a corpus and deeply analyse a given phenomenon.\nLE-CAT was coded by James Tripp. It has been used to support the workshops Youtube as Test Society (University of Siegen), Parking on Twitter (University of Warwick) and the Digital Test of the News (University of Warwick) and is part of the CIM module Digital Object, Digital Methods.\nAcademic correspondence should be sent to Noortje Marres."
  },
  {
    "objectID": "projects/qstep-statistical-workshops.html",
    "href": "projects/qstep-statistical-workshops.html",
    "title": "QSTEP: Statistical workshop",
    "section": "",
    "text": "QSTEP asked me to support their students in their statistical methods modules. The aim was to provide hands on R work with various statistical methods, as used in my Psychology PhD, including:\n\nBivariate regression\nLinear Regression\nMultiple Regression\nMultinomial Logit\nProportional Odds\n\nA focus throughout the sessions were to consider why one model is chosen, what research question it addresses, and how to evaluate the models ability to capture (or failure to capture) the pattern in the data.\nLinks to the rendered output are below. For the original R markdown files see the link above.\n\n\n\nFile\nTopic and HTML link\n\n\n\n\nregression-1-1.Rmd\nBasic introduction to bivariate regression\n\n\nregression-1-2.Rmd\nSimple model evaluation\n\n\nregression-1-3.Rmd\nApplied example using the British Attitudes Survey data\n\n\nregression-2-1.Rmd\nBasic introduction to linear regression\n\n\nregression-2-2.Rmd\nBasic multiple regression\n\n\nregression-2-3.Rmd\nApplied example using the British Attitudes Survey data\n\n\nbridge.Rmd\nSummary/catch-up workshop\n\n\nbridge2.html\nWorking with categorical data"
  },
  {
    "objectID": "projects/introductory-social-media-analysis.html",
    "href": "projects/introductory-social-media-analysis.html",
    "title": "Introductory Social Media Analysis",
    "section": "",
    "text": "Rudimentary example of collecting data from Twitter aimed at introducing R and data collection to humanities students."
  },
  {
    "objectID": "projects/RandDocker.html",
    "href": "projects/RandDocker.html",
    "title": "R and Docker",
    "section": "",
    "text": "Slides\n\n\n\n\nAbstract\nDocker is a container technology which allows you to specify and build a compute environment. You can run the environment on your local machine and both the instructions for creating the environment and the environment itself can be shared. In this talk I will (a) introduce containers, (b) discuss a worked example of using Docker with R, and (c) consider how containers can help us do better research. Hopefully this presentation will help those of you who have never used Docker to consider if you want to use Docker in your own work and where to start."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "James Tripp",
    "section": "",
    "text": "My career has taken me from working as a psychology researcher to serving as an academic technologist and research software engineer at the University of Warwick. I now apply my expertise as a data analytics specialist at the Government Internal Audit Agency."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "James Tripp",
    "section": "",
    "text": "R and Docker\n\n\n\n\n\n\n\nPresentation\n\n\n \n\n\n\n\nSep 15, 2022\n\n\nJames Tripp\n\n\n\n\n\n\n\n\nQuarto: a library to run them all?\n\n\n\n\n\n\n\nPresentation\n\n\n \n\n\n\n\nSep 7, 2022\n\n\nCarlos Cámara-Menoyo, Cagatay Turkay, James Tripp\n\n\n\n\n\n\n\n\nGrapho\n\n\n\n\n\n\n\nSoftware\n\n\n \n\n\n\n\nMay 15, 2022\n\n\nJames Tripp, Gregory McInerny\n\n\n\n\n\n\n\n\nNetworking Workshop\n\n\n\n\n\n\n\nTeaching\n\n\n \n\n\n\n\nFeb 21, 2022\n\n\nJames Tripp, Godwin Yeboah\n\n\n\n\n\n\n\n\nIntroductory Social Media Analysis\n\n\n\n\n\n\n\nTeaching\n\n\n \n\n\n\n\nFeb 1, 2022\n\n\nJames Tripp\n\n\n\n\n\n\n\n\nSQL Masterclass\n\n\n\n\n\n\n\nTeaching\n\n\n \n\n\n\n\nNov 25, 2021\n\n\nJames Tripp\n\n\n\n\n\n\n\n\nBackfillz\n\n\n\n\n\n\n\nSoftware\n\n\n \n\n\n\n\nMay 4, 2020\n\n\nJames Tripp, Gregory McInerny\n\n\n\n\n\n\n\n\nA dilution effect without dilution\n\n\n\n\n\n\n\nPaper\n\n\n \n\n\n\n\nMar 31, 2020\n\n\nAdam N.Sanborn, Takao Noguchi, James Tripp, Neil Stewart\n\n\n\n\n\n\n\n\nLE-CAT: Lexicon-based Categorization and Analysis Tool\n\n\n\n\n\n\n\nSoftware\n\n\n \n\n\n\n\nMar 20, 2020\n\n\nJames Tripp, Noortje Marres\n\n\n\n\n\n\n\n\nQSTEP: Statistical workshop\n\n\n\n\n\n\n\nTeaching\n\n\n \n\n\n\n\nAug 18, 2019\n\n\nJames Tripp\n\n\n\n\n\n\n\n\nMasterclass: Social Media\n\n\n\n\n\n\n\nTeaching\n\n\n \n\n\n\n\nMar 18, 2019\n\n\nJames Tripp\n\n\n\n\n\n\n\n\nBeing paid relatively well most of the time\n\n\n\n\n\n\n\nPaper\n\n\n \n\n\n\n\nMar 31, 2016\n\n\nJames Tripp, Gordon DA Brown\n\n\n\n\n\n\n\n\nMemory and sampling in contextual judgment\n\n\n\n\n\n\n\nPhD Thesis\n\n\n \n\n\n\n\nJan 10, 2013\n\n\nJames Tripp\n\n\n\n\n\n\nNo matching items"
  }
]